* Weakly-supervised learning task (Simplified)

** Intro

Machine learning models are rather opaque - it is often hard to
tell why and how they make the decisions.
Interpretable Machine Learning is an interesting research topic that
aims to build tools decisions of which can be understood by humans.

In this exercise, we will focus on applying the Grad-CAM technique
[[https://arxiv.org/abs/1610.02391]]. By using Grad-CAM, we can see the
regions in the input images where activations have fired most often,
thus contributing to the final prediction.

** Using Grad-CAM for object detection

I have fine-tuned a basic ResNet50 model on the VOC2007 training data.
The model predicts whether the image belongs to one of the 20 VOC
classes. I want to use the network trained for classification to
produce detections.

For this, you need to implement the Grad-CAM technique and produce
box-level predictions using the Grad-CAM outputs. For this task, I will
only ask you to predict dogs and their locations on a subset of the
VOC2007 test set.

Here is an example of the prediction that I get. The predicted box
(dog_1) is not perfect but is roughly in the proper location (GT_dog).

[[./doc/dog_and_gradcam.jpg]]

** What you have

- I provide the code and the model in PyTorch.
  - ~train_voc_classifier.py~ and ~eval_voc_classifier.py~
    respectively train and evaluate on the VOC2007 classification task.
    - This model achieves ~83-85% mAP on the VOC2007 test set.
    - *Hint*: If your PyTorch is rusty, you can use this code as a reference.
  - If you don't have a PyTorch-supported GPU, you will have to run the
    model slowly in CPU mode.
    - This would make the training via ~train_voc_classifier.py~ take ~1-2 hours,
      which is annoying.
    - For this reason, I provide a checkpoint with a trained model:
      - [[https://sharedocs.huma-num.fr/wl/?id=zIfive7ZymGf6vRZJiskKBCFXiOURaeq]]
      - (If the above one does not load):
        - [[https://sharedocs.huma-num.fr/wl/?id=58zGDaRFcxczvZY6RK0K8TDnGKcIHOSw]]
  - ~dog_detection.py~ has two simple baselines and the code
    for evaluation of the detected boxes.
  - *This is a simplified version of the exercise. I provide an additional
    baseline C and some pointers, annotated with <HINT> in the code.*

- These are the baseline results that I get on the first 500 images in
  the VOC2007 test set.
  - Metric is average precision, at different intersection thresholds.

    | Baseline       | Threshold 0.3 | Threshold 0.4 | Threshold 0.5 |
    |----------------+---------------+---------------+---------------|
    | A. CENTERBOX   | ap: 3.868464  | ap: 2.15691   | ap: 0.283798  |
    |                | recall: 54.00 | recall: 38.00 | recall: 16.00 |
    | B. CHEATING    | ap: 40.296106 | ap: 22.182328 | ap: 4.509091  |
    | CENTERBOX      | recall: 54.00 | recall: 38.00 | recall: 16.00 |
    | C. SCORED      | ap: 50.434485 | ap: 27.06006  | ap: 5.821429  |
    | CENTERBOX      | recall: 54.00 | recall: 38.00 | recall: 16.00 |

- Just for reference: rough performance I got with a quick & dirty approach.

    | Threshold      | 0.3           | 0.4           | 0.5           |
    |----------------+---------------+---------------+---------------|
    | ap             | 48.805799     | 43.845483     | 21.4125       |
    | recall         | 58.00         | 54.00         | 34.00         |

** What not to do

- Please avoid submitting Jupyter notebooks.
- Don't just ~from pytorch_grad_cam import GradCAM~.
  - I want to see your code for Grad-CAM.
  - If you are copy-pasting, make sure you understand how it works.
- Obviously, don't use the ground truth labels in your solution.

** What you have to do

1. Read and understand the paper.
2. Implement the dog detection approach via Grad-CAM and achieve
   reasonable performance (see the baselines above).
   - You should write clean and readable code.
   - Show good scientific conduct.
3. Prepare to answer the following questions:

*** How does Grad-CAM work? Difference to CAM

**** What is CAM?

CAM (Class Activation Map) works by **weighting** the **feature maps**
from the last convolutional layer and then combining them to generate
a **heatmap** that highlights important regions.

⚠️ Limitation: CAM only works with CNNs where fully connected layers
are replaced by Global Average Pooling (GAP), making it less flexible.

**** What is Grad-CAM?

Grad-CAM overcomes CAM's limitations by using **gradients** to compute
the **importance weights**, which are then applied to the feature maps
to generate the heatmap.

How does Grad-CAM work?
1. Forward Pass: The input image is fed through the CNN to obtain the
   classification result.
2. Backward Pass: The gradient of the target class (e.g., "how much this
   image looks like a dog") is computed with respect to the feature maps.
3. Weight Calculation: The average of these gradients determines which
   feature maps are most important.
4. Heatmap Generation: These weighted feature maps are combined and
   passed through a ReLU function to produce the final Grad-CAM
   visualization.

**** Grad-CAM vs. CAM Summary

| Comparison          | CAM (Traditional Version) | Grad-CAM (Enhanced Version) |
|---------------------+---------------------------+----------------------------|
| Structural Dependency | Works only with GAP-based CNNs | Works with any CNN       |
| Computation Method  | Uses fixed weights for feature maps | Uses gradients to compute importance weights |
| Flexibility         | Only applicable to classification tasks | Works for classification, detection, and segmentation |
| Interpretability    | Only provides insights for the last convolutional layer | Can explain multiple layers in the network |

In short: Grad-CAM is an improved version of CAM, using gradients to
compute heatmaps, making it more flexible and precise!

*** Your reasoning when implementing the detection approach

Heatmap Generation:
- Select the last convolutional layer of ResNet50, register a hook to
  obtain activation maps and gradients.
- Generate heatmaps through gradient weighting.

Post-processing to generate detection boxes:
- Threshold the heatmap (e.g., 0.5), extract contours, and filter out
  small regions (e.g., <30 pixels).
- Map the heatmap coordinates back to the original image size to
  generate detection boxes.

Combine with classification scores:
- Use the dog category score predicted by the classification model as
  the confidence for the detection box.
- Apply NMS to merge overlapping boxes.

#+BEGIN_SRC mermaid
graph TD
    A[Input Image] --> B[Image Preprocessing]
    B --> C[Resize to 224x224]
    C --> D["Normalization: MEAN=[0.485,0.456,0.406], STD=[0.229,0.224,0.225]"]
    D --> E[Convert to Tensor]

    E --> F[ResNet50 Forward Propagation]
    F -->|Existing Code| G[Get Classification Prediction Score]
    G --> H[Baseline C: Generate Center Box<br>scale=0.3 square region]

    F -->|To be Implemented| I[Grad-CAM Core Process]
    I --> J[Register Forward Hook<br>Capture Activation of Last Layer in layer4]
    I --> K[Register Backward Hook<br>Capture Gradient of Target Class]
    J --> L[Save Activation Feature Map]
    K --> M[Save Gradient Feature Map]
    L --> N[Calculate Global Average Weights of Gradients]
    M --> N
    N --> O[Generate Weighted Heatmap]
    O --> P[ReLU Activation Filters Negative Values]
    P --> Q[Bilinear Interpolation Upsample to Original Image Size]

    Q --> R[Heatmap Post-processing]
    R --> S[Normalize Heatmap to 0-1 Range]
    S --> T["Threshold Processing (threshold=0.5)"]
    T --> U[Contour Detection findContours]
    U --> V[Generate Candidate Bounding Boxes]
    V --> W[Non-Maximum Suppression NMS]
    W --> X[Select Box with Highest Confidence]

    X -->|Replace Baseline C| Y[New Detection Box + Classification Score]
    Y --> Z[Evaluation Metric Calculation]
    Z --> AA[Output AP/Recall Results]

    style H fill:#f9f,stroke:#333
    style I fill:#7f7,stroke:#333
    style J fill:#7f7,stroke:#333
    style K fill:#7f7,stroke:#333
    style R fill:#7f7,stroke:#333
    style Y fill:#7f7,stroke:#333

    subgraph Existing Code Module
        B
        C
        D
        E
        F
        G
        H
        Z
        AA
    end

    subgraph Grad-CAM Module to be Implemented
        I
        J
        K
        L
        M
        N
        O
        P
        Q
        R
        S
        T
        U
        V
        W
        X
        Y
    end
#+END_SRC

*** Explain the metrics used: What is Recall, Average Precision (AP)?

Key Metrics in Dog Detection:

1. **Recall**: Measures how many actual dogs were correctly detected.

   \[
   \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
   \]

   - High Recall → Few missed detections.
   - Low Recall → Many dogs were not detected.

2. **Average Precision (AP)**: Summarizes the precision-recall tradeoff
   by computing the **area under the PR curve**.
   - High AP → Good balance between detecting dogs and avoiding false alarms.

In the "dog_detection":
- `eval_stats_at_threshold()` calculates recall & AP.
- Model's predictions are compared with ground truth.

Both **Recall & AP** ensure the model **finds most dogs** while minimizing
false detections!

*** How would you improve the method if you had more time?

Optimized Post-processing Parameters:
- Adaptive Threshold Adjustment: Adjust thresholds dynamically based on
  image content (e.g., use higher thresholds for high-contrast images).
- Multi-scale Detection: Generate heatmaps at different resolutions to
  improve small object detection capabilities.

Model Improvements:
- Grad-CAM++: Refine gradient weighting methods to reduce noise and
  enhance heatmap quality (so the heatmaps won't appear as a blob).

Engineering Optimizations:
- Parallel Computing: Leverage GPU acceleration for heatmap generation
  (my GPU environment seems to have been failing to configure) and NMS
  calculations.
- Model Quantization: Reduce model size and improve inference speed.